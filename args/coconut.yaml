# ==========================
# CoCoNuT (Latent Curriculum) Training Config
# ==========================
# Use this config for staged CoCoNuT training (with latent tokens).
# The model is trained with a curriculum, gradually replacing reasoning steps with latent tokens.
# Use with run_coconut.py

project: multicoco-coconut                # W&B project name
name: multicoco-coconut-stage1plus        # W&B run name
model_id: gpt2-xl                         # Huggingface model name for GPT-2
clip_id: openai/clip-vit-base-patch32     # Huggingface model name for CLIP

# === Data paths ===
train_path: data/Datasets/A-OKVQA/aokvqa_train.json
val_path: data/Datasets/A-OKVQA/aokvqa_validation.json

# === Checkpointing ===
save_path: checkpoints/coconut_aokvqa     # Where to save checkpoints

# === Training hyperparameters ===
batch_size_training: 4
batch_size_eval: 4
num_epochs: 20
lr: 2e-5
weight_decay: 0.01
seed: 42
resume: 0
max_length: 256                           # Max sequence length for GPT-2

# === CoCoNuT-specific curriculum settings ===
c_thought: 2              # Number of latent tokens per replaced reasoning step
max_latent_stage: 4       # Maximum number of reasoning steps to replace with latents
epochs_per_stage: 4       # Number of epochs per curriculum stage

# === Pretrained CoT checkpoint ===
load_model_path: checkpoints/cot_aokvqa/best_cot.pt  # Path to best CoT checkpoint 