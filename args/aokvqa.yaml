name: run_1_aokvqa_baseline
model_id: gpt2
seed: 42
bf16: false  # Not using bf16; using fp16 (via AMP)
project: Coconut_Aokvqa_Guna_LLaVA-1.5-7B

# ‚úÖ Save checkpoints to Google Drive (persistent)
save_path: /content/drive/MyDrive/COCONUT/checkpoints/coconut_aokvqa_gunadeep
load_model_path: null
resume: 0
reset_optimizer: false
only_eval: false
save_only_improve: true

# ‚úÖ Dataset paths (adjust if needed)
train_path: /content/MultiModal-COCONUT-GPT2-CLIP-/data/Datasets/A-OKVQA/aokvqa_train.json
val_path: /content/MultiModal-COCONUT-GPT2-CLIP-/data/Datasets/A-OKVQA/aokvqa_validation.json

# ‚õìÔ∏è Reasoning setup
debug: false
no_thoughts: false
cot: false
no_cot: false
coconut: true

# üß† Curriculum configs
c_thought: 1             # Try 1 for fewer latent tokens per step
max_latent_stage: 2      # Try 2 or 4 for easier start
epochs_per_stage: 4      # More epochs per stage for slower curriculum
pad_latent_to_max: true
uniform_prob: 0.1

# üß† Latent-space configs (REQUIRED for EM-style training)
latent_dim: 768           # GPT-2 base hidden size
n_latents: 4              # Number of latent tokens per sample
latent_lr: 1e-3           # Learning rate for latent vectors (E-step)
e_steps: 3                # Number of E-step optimization steps per batch

# üèãÔ∏è Training configs for Colab Free Tier
num_epochs: 40
batch_size_training: 32         # Increased for A100 GPU (was 8)
gradient_accumulation_steps: 1 # No accumulation needed for A100, batch size 32
lr: 1e-4
weight_decay: 0.0

max_length: 512  # Optimal for A-OKVQA, fits all data, saves memory
